[{"content":"At Baania, we use SOPS to check in encrypted secrets into git repos. This solves plaintext credentials in version control. However, say, you have 5 repos using the same database credentials, rotating secrets means you have to go into each repo and update the SOPS credentials manually.\nAlso worth nothing that, for GitHub actions, authenticating AWS means you have to add repo secrets. This means for all the repos you have CI enabled, you have to populate the repo secrets with AWS credentials. When time comes for rotating the creds, you\u0026rsquo;ll encounter the same situation as above.\nI did some research and consensus for AWS / Terraform setup is to: encrypt secrets via SOPS, and use Terraform to create AWS SSM secret entries. That way, you have a trail for credentials. This setup means:\n You don\u0026rsquo;t have to populate repos with AWS creds, instead supplying an ARN role instead. You don\u0026rsquo;t have to change credentials in projects, since they all get the secrets from AWS SSM.  Implementation Repo here: https://github.com/baania/terraform-sops-ssm\n1. Bootstrap Terraform terraform { required_providers { sops = { source = \u0026#34;carlpett/sops\u0026#34; version = \u0026#34;0.6.3\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-southeast-1\u0026#34; profile = \u0026#34;playground\u0026#34; } provider \u0026#34;sops\u0026#34; {} terraform { required_version = \u0026#34;\u0026gt;= 1.0\u0026#34; } 2. Create KMS key for SOPS https://github.com/mozilla/sops/#kms-aws-profiles\nresource \u0026#34;aws_kms_key\u0026#34; \u0026#34;sops\u0026#34; { description = \u0026#34;Keys to decrypt SOPS encrypted values\u0026#34; } resource \u0026#34;aws_kms_alias\u0026#34; \u0026#34;sops\u0026#34; { name = \u0026#34;alias/sops\u0026#34; target_key_id = aws_kms_key.sops.key_id } 3. Create SSM secrets Create a folder named secrets, inside it create JSON files and encrypt each with sops.\nlocals { secrets = toset([ \u0026#34;db-foo\u0026#34;, ]) } data \u0026#34;sops_file\u0026#34; \u0026#34;sops_secrets\u0026#34; { for_each = local.secrets source_file = \u0026#34;secrets/${each.key}.sops.json\u0026#34; }# aws keeps the secrets for 7 days before actual deletion. consider using random names during test resource \u0026#34;aws_secretsmanager_secret\u0026#34; \u0026#34;ssm_secrets\u0026#34; { for_each = local.secrets name = each.key } resource \u0026#34;aws_secretsmanager_secret_version\u0026#34; \u0026#34;ssm_secrets\u0026#34; { for_each = local.secrets secret_id = aws_secretsmanager_secret.ssm_secrets[\u0026#34;${each.key}\u0026#34;].id secret_string = jsonencode(data.sops_file.sops_secrets[\u0026#34;${each.key}\u0026#34;].data) } 4. Create IAM policy for SSM access data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;secrets_ro\u0026#34; { statement { actions = [ \u0026#34;secretsmanager:GetResourcePolicy\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34; ] resources = [ \u0026#34;arn:aws:secretsmanager:ap-southeast-1:$AWS_ACCOUNT_ID:secret:*\u0026#34;, ] } statement { actions = [ \u0026#34;secretsmanager:ListSecrets\u0026#34; ] resources = [\u0026#34;*\u0026#34;] } } resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;secrets_ro\u0026#34; { name = \u0026#34;secrets_ro\u0026#34; path = \u0026#34;/\u0026#34; policy = data.aws_iam_policy_document.secrets_ro.json } 5. Create IAM user for local dev You shouldn\u0026rsquo;t supply AWS credentials for deployment, since you can grant access via IAM roles instead.\nresource \u0026#34;aws_iam_user\u0026#34; \u0026#34;playground-prod-dev\u0026#34; { name = \u0026#34;playground-prod-dev\u0026#34; path = \u0026#34;/users/\u0026#34; } resource \u0026#34;aws_iam_access_key\u0026#34; \u0026#34;playground-prod-dev\u0026#34; { user = aws_iam_user.playground-prod-dev.name } Grant IAM user access to SSM resource \u0026#34;aws_iam_user_policy_attachment\u0026#34; \u0026#34;playground-prod-dev\u0026#34; { user = aws_iam_user.playground-prod-dev.name for_each = toset([ aws_iam_policy.secrets_ro.arn, ]) policy_arn = each.value } 6. Create IAM role for Lambda resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda_role\u0026#34; { name = \u0026#34;lambda_role\u0026#34; path = \u0026#34;/sa/\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34; : { \u0026#34;Service\u0026#34; : \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34; : \u0026#34;sts:AssumeRole\u0026#34; }, ] }) managed_policy_arns = [ aws_iam_policy.secrets_ro.arn, ] inline_policy { name = \u0026#34;create_cloudwatch_logs\u0026#34; policy = jsonencode({ \u0026#34;Version\u0026#34; : \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34; : [ { \u0026#34;Action\u0026#34; : [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34; : \u0026#34;*\u0026#34; }, ] }) } } 7. Create IAM role for GitHub actions Need to create OIDC so GitHub can assume AWS roles\nresource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;github\u0026#34; { url = \u0026#34;https://token.actions.githubusercontent.com\u0026#34; client_id_list = [\u0026#34;sts.amazonaws.com\u0026#34;] thumbprint_list = [\u0026#34;a031c46782e6e6c662c2c87c76da9aa62ccabd8e\u0026#34;] } Assume role policy is on per-repo basis\nlocals { repositories = [ \u0026#34;terraform-sops-ssm\u0026#34;, ] } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;github_actions_assume_role\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;] principals { type = \u0026#34;Federated\u0026#34; identifiers = [aws_iam_openid_connect_provider.github.arn] } condition { test = \u0026#34;ForAnyValue:StringLike\u0026#34; variable = \u0026#34;token.actions.githubusercontent.com:sub\u0026#34; values = [for v in local.repositories : \u0026#34;repo:$organization/${v}:*\u0026#34;] } } } Finally attach the above policy to role\nresource \u0026#34;aws_iam_role\u0026#34; \u0026#34;playground-prod-github\u0026#34; { name = \u0026#34;playground-prod-github\u0026#34; path = \u0026#34;/sa/\u0026#34; assume_role_policy = data.aws_iam_policy_document.github_actions_assume_role.json managed_policy_arns = [ aws_iam_policy.secrets_ro.arn, ] } The end ðŸŽ‰\nBonus  GitHub actions with AWS login AWS Lambda with SSM template  ","permalink":"https://blog.baania.com/posts/secrets-management-with-sops-aws-ssm-terraform/","summary":"At Baania, we use SOPS to check in encrypted secrets into git repos. This solves plaintext credentials in version control. However, say, you have 5 repos using the same database credentials, rotating secrets means you have to go into each repo and update the SOPS credentials manually.\nAlso worth nothing that, for GitHub actions, authenticating AWS means you have to add repo secrets. This means for all the repos you have CI enabled, you have to populate the repo secrets with AWS credentials.","title":"Secrets management with SOPS, AWS SSM and Terraform"},{"content":"Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don\u0026rsquo;t need to throw some data away, just have to impute them.\nImputation, essentially means \u0026ldquo;replacing missing data with substituted values.\u0026rdquo; Good imputation can improve your model performance, but bad ones can cause a catastrophe. Thus, it should be done carefully and thoughtfully. The process can be complex at times, luckily we have a few python modules that can make the process easier!\nBelow are steps you can take in order to create an imputation pipeline. Github link here!\nfrom random import randint import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error, median_absolute_error from hyperopt import fmin, tpe, hp, Trials, STATUS_OK import mlflow import matplotlib.pyplot as plt import seaborn as sns sns.set() Generate data Since this is an example and I don\u0026rsquo;t want to get sued by using my company\u0026rsquo;s data, synthetic data it is :) This simulates a dataset from different pseudo-regions, with different characteristics. Real data will be much more varied, but I make it more obvious so it\u0026rsquo;s easy to see the differences.\ndef generate_array_with_random_nan(lower_bound, upper_bound, size): a = np.random.randint(lower_bound, upper_bound+1, size=size).astype(float) mask = np.random.choice([1, 0], a.shape, p=[.1, .9]).astype(bool) a[mask] = np.nan return a size = 6000 df_cbd = pd.DataFrame() df_cbd[\u0026#39;bed\u0026#39;] = generate_array_with_random_nan(1, 2, size) df_cbd[\u0026#39;bath\u0026#39;] = generate_array_with_random_nan(1, 2, size) df_cbd[\u0026#39;area_usable\u0026#39;] = np.random.randint(20, 40, size=size) df_cbd[\u0026#39;region\u0026#39;] = \u0026#39;cbd\u0026#39; df_suburb = pd.DataFrame() df_suburb[\u0026#39;bed\u0026#39;] = generate_array_with_random_nan(1, 4, size) df_suburb[\u0026#39;bath\u0026#39;] = generate_array_with_random_nan(1, 4, size) df_suburb[\u0026#39;area_usable\u0026#39;] = np.random.randint(30, 200, size=size) df_suburb[\u0026#39;region\u0026#39;] = \u0026#39;suburb\u0026#39; df = pd.concat([df_cbd, df_suburb]) df     bed bath area_usable region     0 2 1 33 cbd   1 1 2 23 cbd   2 1 2 33 cbd   3 2 1 26 cbd   4 2 1 28 cbd   5 2 2 36 cbd   6 1 2 38 cbd   7 2 1 23 cbd   8 2 1 36 cbd   9 nan 2 29 cbd    Report missing values I also randomly remove some values to mimic real-world data (read: they are never ready to use), here we will visualize the missing rate of each column.\ndef report_missing(df): cnts = [] cnt_total = len(df) for col in df.columns: cnt_missing = sum(pd.isnull(df[col]) | pd.isna(df[col])) print(\u0026#34;col: {}, missing: {}%\u0026#34;.format(col, 100.0 * cnt_missing / cnt_total)) cnts.append({ \u0026#39;column\u0026#39;: col, \u0026#39;missing\u0026#39;: 100.0 * cnt_missing / cnt_total }) cnts_df = pd.DataFrame(cnts) sns.barplot(x=cnts_df.missing, y=cnts_df.column, # palette=[\u0026#39;r\u0026#39;,\u0026#39;b\u0026#39;], # data=cnts_df ) return sns report_missing(df) col: bed, missing: 10.266666666666667% col: bath, missing: 9.616666666666667% col: area_usable, missing: 0.0% col: region, missing: 0.0% Data exploration Knowing the missing rate isn\u0026rsquo;t everything, thus it is also a good idea to explore data in other areas too.\n## missing bed per region df[df.bed.isna()][\u0026#34;region\u0026#34;].value_counts(dropna=False) cbd 634 suburb 598 Name: region, dtype: int64  ## missing bath per region df[df.bath.isna()][\u0026#34;region\u0026#34;].value_counts(dropna=False) suburb 588 cbd 566 Name: region, dtype: int64  ## explore region df.region.value_counts() suburb 6000 cbd 6000 Name: region, dtype: int64  ## explore bed df.bed.value_counts() 2.0 4050 1.0 4009 4.0 1393 3.0 1316 Name: bed, dtype: int64  ## explore bath df.bath.value_counts() 1.0 4142 2.0 4022 3.0 1393 4.0 1289 Name: bath, dtype: int64  Remove outliers (wouldn\u0026rsquo;t want your model to have a sub-par performance from skewed data :-P)\n## remove outliers here Create synthetic columns In this step, we create percentile, mean and rank columns to add more data points, so the model can perform better :D\nFirst, we find aggregate percentiles for each groupby set, then add mean and rank columns.\nsynth_columns = { \u0026#39;bed\u0026#39;: { \u0026#34;region_bath\u0026#34;: [\u0026#39;region\u0026#39;, \u0026#39;bath\u0026#39;] }, \u0026#39;bath\u0026#39;: { \u0026#34;region_bed\u0026#34;: [\u0026#39;region\u0026#39;, \u0026#39;bed\u0026#39;] } } for column, groupby_levels in synth_columns.items(): for groupby_level_name, groupby_columns in groupby_levels.items(): # percentile aggregates for pctl in [20,50,80,90]: col_name = \u0026#39;p{}|{}|{}\u0026#39;.format(pctl, groupby_level_name, column) print(\u0026#34;calculating -- {}\u0026#34;.format(col_name)) df[col_name] = df[groupby_columns+[column]].fillna(0).groupby(groupby_columns)[column].transform(lambda x: x.quantile(pctl/100.0)) # mean impute mean_impute = \u0026#39;mean|{}|{}\u0026#39;.format(groupby_level_name,column) print(\u0026#34;calculating -- {}\u0026#34;.format(mean_impute)) df[mean_impute] = df.groupby(groupby_columns)[column].transform(\u0026#39;mean\u0026#39;) # bed/bath rank rank_impute = column_name = \u0026#39;rank|{}|{}\u0026#39;.format(groupby_level_name,column) print(\u0026#34;calculating -- {}\u0026#34;.format(rank_impute)) df[rank_impute] = df.groupby(groupby_columns)[column].rank(method=\u0026#39;dense\u0026#39;, na_option=\u0026#39;bottom\u0026#39;) calculating -- p20|region_bath|bed calculating -- p50|region_bath|bed calculating -- p80|region_bath|bed calculating -- p90|region_bath|bed calculating -- mean|region_bath|bed calculating -- rank|region_bath|bed calculating -- p20|region_bed|bath calculating -- p50|region_bed|bath calculating -- p80|region_bed|bath calculating -- p90|region_bed|bath calculating -- mean|region_bed|bath calculating -- rank|region_bed|bath  Coalesce values In this step we fill in values obtained from the previous step \u0026ndash; impute time!!\ndef coalesce(df, columns): \u0026#39;\u0026#39;\u0026#39; Implement coalesce of function in colunms. Inputs: df: reference dataframe columns: columns to perform coalesce Returns: df_tmp: pd.Series that is coalesced Example: df_tmp = pd.DataFrame({\u0026#39;a\u0026#39;: [1,2,None,None,None,None], \u0026#39;b\u0026#39;: [None,6,None,8,9,None], \u0026#39;c\u0026#39;: [None,10,None,12,None,13]}) df_tmp[\u0026#39;new\u0026#39;] = coalesce(df_tmp, [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;]) print(df_tmp) \u0026#39;\u0026#39;\u0026#39; df_tmp = df[columns[0]] for c in columns[1:]: df_tmp = df_tmp.fillna(df[c]) return df_tmp coalesce_columns = [ \u0026#39;bed\u0026#39;, \u0026#39;p50|region_bath|bed\u0026#39;, # p50|GROUPBY_LESSER_WEIGHT|bed, ... ] df[\u0026#34;bed_imputed\u0026#34;] = coalesce(df, coalesce_columns) coalesce_columns = [ \u0026#39;bath\u0026#39;, \u0026#39;p50|region_bed|bath\u0026#39;, # p50|GROUPBY_LESSER_WEIGHT|bath, ... ] df[\u0026#34;bath_imputed\u0026#34;] = coalesce(df, coalesce_columns) Report missing values (again) After we impute the values, let\u0026rsquo;s see how much we are doing better!\nreport_missing(df) col: bed, missing: 10.266666666666667% col: bath, missing: 9.616666666666667% col: area_usable, missing: 0.0% col: region, missing: 0.0% col: p20|region_bath|bed, missing: 0.0% col: p50|region_bath|bed, missing: 0.0% col: p80|region_bath|bed, missing: 0.0% col: p90|region_bath|bed, missing: 0.0% col: mean|region_bath|bed, missing: 9.616666666666667% col: rank|region_bath|bed, missing: 0.0% col: p20|region_bed|bath, missing: 0.0% col: p50|region_bed|bath, missing: 0.0% col: p80|region_bed|bath, missing: 0.0% col: p90|region_bed|bath, missing: 0.0% col: mean|region_bed|bath, missing: 10.266666666666667% col: rank|region_bed|bath, missing: 0.0% col: bed_imputed, missing: 0.0% col: bath_imputed, missing: 0.0%  Notice that the imputed columns there are no missing values. Yay!\nAssign partition In this step, we partition the data into three sets: train, dev and test. Normally we only split into train and test set, but the additional \u0026ldquo;dev\u0026rdquo; set is there so we can make sure it\u0026rsquo;s not too overfit or underfit.\n## assign partition def assign_partition(x): if x in [0,1,2,3,4,5]: return 0 elif x in [6,7]: return 1 else: return 2 ## assign random id df[\u0026#39;listing_id\u0026#39;] = [randint(1000000, 9999999) for i in range(len(df))] ## hashing df[\u0026#34;hash_id\u0026#34;] = df[\u0026#34;listing_id\u0026#34;].apply(lambda x: x % 10) ## assign partition df[\u0026#34;partition_id\u0026#34;] = df[\u0026#34;hash_id\u0026#34;].apply(lambda x: assign_partition(x)) ## define columns group y_column = \u0026#39;area_usable\u0026#39; categ_columns = [\u0026#39;region\u0026#39;] numer_columns = [ \u0026#39;bed_imputed\u0026#39;, \u0026#39;bath_imputed\u0026#39;, \u0026#39;p20|region_bath|bed\u0026#39;, \u0026#39;p50|region_bath|bed\u0026#39;, \u0026#39;p80|region_bath|bed\u0026#39;, \u0026#39;p90|region_bath|bed\u0026#39;, \u0026#39;mean|region_bath|bed\u0026#39;, \u0026#39;rank|region_bath|bed\u0026#39;, \u0026#39;p20|region_bed|bath\u0026#39;, \u0026#39;p50|region_bed|bath\u0026#39;, \u0026#39;p80|region_bed|bath\u0026#39;, \u0026#39;p90|region_bed|bath\u0026#39;, \u0026#39;mean|region_bed|bath\u0026#39;, \u0026#39;rank|region_bed|bath\u0026#39;, ] id_columns = [ \u0026#39;listing_id\u0026#39;, \u0026#39;hash_id\u0026#39;, \u0026#39;partition_id\u0026#39; ] ## remove missing y df = df.dropna(subset=[y_column]) ## split into train-dev-test df_train = df[df[\u0026#34;partition_id\u0026#34;] == 0] df_dev = df[df[\u0026#34;partition_id\u0026#34;] == 1] df_test = df[df[\u0026#34;partition_id\u0026#34;] == 2] ## split each set into x and y y_train = df_train[y_column].values df_train = df_train[numer_columns+categ_columns] y_dev = df_dev[y_column].values df_dev = df_dev[numer_columns+categ_columns] y_test = df_test[y_column].values df_test = df_test[numer_columns+categ_columns] Create sklearn pipelines Sklearn pipelines is a set of transformers applied on data, plus a final estimator (for statistical model). For instance, you have a transformation step to encode categorical values to integers, if you do this from scratch every time, there\u0026rsquo;s no guarantee that the integers mapping would be the same. Using a pipelines ensures that it retains the same value-integer mapping for subsequent runs. Additionally, applying each transformation and estimators manually each time can be time consuming, and prone to errors. Think of it as a shortcut to do steps required for a task.\nIn this step, we chain a few pipelines together to process the dataset for the final time. In this example, we use median followed by standard scalar for numeric columns, and mode followed by encoding labels for categorical columns.\n## define pipelines impute_median = SimpleImputer(strategy=\u0026#39;median\u0026#39;) impute_mode = SimpleImputer(strategy=\u0026#39;most_frequent\u0026#39;) num_pipeline = Pipeline([ (\u0026#39;impute_median\u0026#39;, impute_median), (\u0026#39;std_scaler\u0026#39;, StandardScaler()), ]) categ_pipeline = Pipeline([ (\u0026#39;impute_mode\u0026#39;, impute_mode), (\u0026#39;categ_1hot\u0026#39;, OneHotEncoder(handle_unknown=\u0026#39;ignore\u0026#39;)), ]) full_pipeline = ColumnTransformer([ (\u0026#34;num\u0026#34;, num_pipeline, numer_columns), (\u0026#34;cat\u0026#34;, categ_pipeline, categ_columns), ]) ## fit and transform X_train = full_pipeline.fit_transform(df_train) X_dev = full_pipeline.transform(df_dev) X_test = full_pipeline.transform(df_test) X_train array([[ 0.04673184, 0.06391404, 0. , ..., -0.16000115, 1. , 0. ], [-0.97000929, -0.97263688, 0. , ..., -1.01065389, 1. , 0. ], [ 0.04673184, 0.06391404, 0. , ..., -0.16000115, 1. , 0. ], ..., [-0.97000929, 1.10046497, 0. , ..., 0.69065159, 0. , 1. ], [ 0.04673184, 1.10046497, 0. , ..., 0.69065159, 0. , 1. ], [ 1.06347297, 2.13701589, 0. , ..., 1.54130432, 0. , 1. ]])  Hyperparameter tuning In this step, we try to use different models and parameters to see which performs the best. We utilize mlflow for logging and hyperopt to help with tuning. In this example, we run the trials for 40 iterations, each using a different combination of model and parameters.\n## mlflow + hyperopt combo def objective(params): regressor_type = params[\u0026#39;type\u0026#39;] del params[\u0026#39;type\u0026#39;] if regressor_type == \u0026#39;gradient_boosting_regression\u0026#39;: estimator = GradientBoostingRegressor(**params) elif regressor_type == \u0026#39;random_forest_regression\u0026#39;: estimator = RandomForestRegressor(**params) elif regressor_type == \u0026#39;extra_trees_regression\u0026#39;: estimator = ExtraTreesRegressor(**params) elif regressor_type == \u0026#39;decision_tree_regression\u0026#39;: estimator = DecisionTreeRegressor(**params) else: return 0 estimator.fit(X_train, y_train) # mae y_dev_hat = estimator.predict(X_dev) mae = median_absolute_error(y_dev, y_dev_hat) # logging with mlflow.start_run(): mlflow.log_param(\u0026#34;regressor\u0026#34;, estimator.__class__.__name__) # mlflow.log_param(\u0026#34;params\u0026#34;, params) mlflow.log_param(\u0026#39;n_estimators\u0026#39;, params.get(\u0026#39;n_estimators\u0026#39;)) mlflow.log_param(\u0026#39;max_depth\u0026#39;, params.get(\u0026#39;max_depth\u0026#39;)) mlflow.log_metric(\u0026#34;median_absolute_error\u0026#34;, mae) return {\u0026#39;loss\u0026#39;: mae, \u0026#39;status\u0026#39;: STATUS_OK} space = hp.choice(\u0026#39;regressor_type\u0026#39;, [ { \u0026#39;type\u0026#39;: \u0026#39;gradient_boosting_regression\u0026#39;, \u0026#39;n_estimators\u0026#39;: hp.choice(\u0026#39;n_estimators1\u0026#39;, range(100,200,50)), \u0026#39;max_depth\u0026#39;: hp.choice(\u0026#39;max_depth1\u0026#39;, range(10,13,1)) }, { \u0026#39;type\u0026#39;: \u0026#39;random_forest_regression\u0026#39;, \u0026#39;n_estimators\u0026#39;: hp.choice(\u0026#39;n_estimators2\u0026#39;, range(100,200,50)), \u0026#39;max_depth\u0026#39;: hp.choice(\u0026#39;max_depth2\u0026#39;, range(3,25,1)), \u0026#39;n_jobs\u0026#39;: -1 }, { \u0026#39;type\u0026#39;: \u0026#39;extra_trees_regression\u0026#39;, \u0026#39;n_estimators\u0026#39;: hp.choice(\u0026#39;n_estimators3\u0026#39;, range(100,200,50)), \u0026#39;max_depth\u0026#39;: hp.choice(\u0026#39;max_depth3\u0026#39;, range(3,10,2)) }, { \u0026#39;type\u0026#39;: \u0026#39;decision_tree_regression\u0026#39;, \u0026#39;max_depth\u0026#39;: hp.choice(\u0026#39;max_depth4\u0026#39;, range(3,10,2)) } ]) trials = Trials() max_evals = 40 best = fmin( fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials) print(\u0026#34;Found minimum after {}trials:\u0026#34;.format(max_evals)) from pprint import pprint pprint(best) 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:19\u0026lt;00:00, 2.11trial/s, best loss: 8.569474762575908] Found minimum after 40 trials: {'max_depth2': 1, 'n_estimators2': 1, 'regressor_type': 1}  Evaluate performance Run \u0026ldquo;mlflow server\u0026rdquo; to see the loggin dashboard. There, we can see that RandomForestRegressor has the best performance (the less MAE the better) when using max_depth=4 and n_estimators=150, to test the model\u0026rsquo;s performance against another test set:\n## use best params on TEST set estimator = RandomForestRegressor(max_depth=4, n_estimators=150) estimator.fit(X_train, y_train) y_train_hat = estimator.predict(X_train) train_mae = median_absolute_error(y_train, y_train_hat) y_dev_hat = estimator.predict(X_dev) dev_mae = median_absolute_error(y_dev, y_dev_hat) y_test_hat = estimator.predict(X_test) test_mae = median_absolute_error(y_test, y_test_hat) mae = { \u0026#39;name\u0026#39;: estimator.__class__.__name__, \u0026#39;train_mae\u0026#39;: train_mae, \u0026#39;dev_mae\u0026#39;: dev_mae, \u0026#39;test_mae\u0026#39;: test_mae } mae = pd.DataFrame([mae]).set_index(\u0026#39;name\u0026#39;) mae    name train_mae dev_mae test_mae     DecisionTreeRegressor 8.930245 8.592484 8.729826    You\u0026rsquo;ll notice that we use \u0026ldquo;median absolute error\u0026rdquo; to measure performance. There are other metrics available, such as mean squared error, but in some cases it\u0026rsquo;s more meaningful to use a metric that measure the performance in actual data\u0026rsquo;s unit, in this case the error on dev and test set are around 8 units away from its correct value. Since normally we use square meter for area, it means the prediction will be off by about 8 square meters in most cases.\nTo summarize, in this article we try to fill in missing data using statistical methods, utilizing sklearn pipelines for reusable steps and hyperparameter tuning to find optimum configuration for imputation process.\n Baania is an equal opportunity company and our team is recruiting. We respect and seek to empower each individual and support the diverse backgrounds, perspectives, skills and experiences.\n","permalink":"https://blog.baania.com/posts/impute-pipelines/","summary":"Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don\u0026rsquo;t need to throw some data away, just have to impute them.\nImputation, essentially means \u0026ldquo;replacing missing data with substituted values.\u0026rdquo; Good imputation can improve your model performance, but bad ones can cause a catastrophe. Thus, it should be done carefully and thoughtfully. The process can be complex at times, luckily we have a few python modules that can make the process easier!","title":"Impute pipelines"}]