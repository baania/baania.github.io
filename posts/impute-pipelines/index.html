<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Impute pipelines | Baania Engineering</title>
<meta name=keywords content="data science">
<meta name=description content="Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don&rsquo;t need to throw some data away, just have to impute them.
Imputation, essentially means &ldquo;replacing missing data with substituted values.&rdquo; Good imputation can improve your model performance, but bad ones can cause a catastrophe. Thus, it should be done carefully and thoughtfully. The process can be complex at times, luckily we have a few python modules that can make the process easier!">
<meta name=author content="Karn Wong">
<link rel=canonical href=https://blog.baania.com/posts/impute-pipelines/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.4efd4e52043b244eb8c9b4fdf959473534d86c9f61ab9d1fec79c7bb38c5a793.css integrity="sha256-Tv1OUgQ7JE64ybT9+VlHNTTYbJ9hq50f7HnHuzjFp5M=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://raw.githubusercontent.com/baania/baania.github.io/master/static/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href="https://github.com/baania/baania.github.io/blob/master/static/favicon-16x16.png?raw=true">
<link rel=icon type=image/png sizes=32x32 href="https://github.com/baania/baania.github.io/blob/master/static/favicon-32x32.png?raw=true">
<link rel=apple-touch-icon href="https://github.com/baania/baania.github.io/blob/master/static/apple-touch-icon.png?raw=true">
<link rel=mask-icon href=https://blog.baania.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.90.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-K0JRY8QCDB"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-K0JRY8QCDB',{anonymize_ip:!1})}</script>
<meta property="og:title" content="Impute pipelines">
<meta property="og:description" content="Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don&rsquo;t need to throw some data away, just have to impute them.
Imputation, essentially means &ldquo;replacing missing data with substituted values.&rdquo; Good imputation can improve your model performance, but bad ones can cause a catastrophe. Thus, it should be done carefully and thoughtfully. The process can be complex at times, luckily we have a few python modules that can make the process easier!">
<meta property="og:type" content="article">
<meta property="og:url" content="https://blog.baania.com/posts/impute-pipelines/"><meta property="og:image" content="https://blog.baania.com/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-12-08T17:00:00+00:00">
<meta property="article:modified_time" content="2021-12-08T17:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://blog.baania.com/papermod-cover.png">
<meta name=twitter:title content="Impute pipelines">
<meta name=twitter:description content="Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don&rsquo;t need to throw some data away, just have to impute them.
Imputation, essentially means &ldquo;replacing missing data with substituted values.&rdquo; Good imputation can improve your model performance, but bad ones can cause a catastrophe. Thus, it should be done carefully and thoughtfully. The process can be complex at times, luckily we have a few python modules that can make the process easier!">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.baania.com/posts/"},{"@type":"ListItem","position":2,"name":"Impute pipelines","item":"https://blog.baania.com/posts/impute-pipelines/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Impute pipelines","name":"Impute pipelines","description":"Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don\u0026rsquo;t need to throw some data away, just have to impute them.\nImputation, essentially means \u0026ldquo;replacing missing data with substituted values.\u0026rdquo; Good imputation can improve your model performance, but bad ones can cause a catastrophe. Thus, it should be done carefully and thoughtfully. The process can be complex at times, luckily we have a few python modules that can make the process easier!","keywords":["data science"],"articleBody":"Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don’t need to throw some data away, just have to impute them.\nImputation, essentially means “replacing missing data with substituted values.” Good imputation can improve your model performance, but bad ones can cause a catastrophe. Thus, it should be done carefully and thoughtfully. The process can be complex at times, luckily we have a few python modules that can make the process easier!\nBelow are steps you can take in order to create an imputation pipeline. Github link here!\nfrom random import randint import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error, median_absolute_error from hyperopt import fmin, tpe, hp, Trials, STATUS_OK import mlflow import matplotlib.pyplot as plt import seaborn as sns sns.set() Generate data Since this is an example and I don’t want to get sued by using my company’s data, synthetic data it is :) This simulates a dataset from different pseudo-regions, with different characteristics. Real data will be much more varied, but I make it more obvious so it’s easy to see the differences.\ndef generate_array_with_random_nan(lower_bound, upper_bound, size): a = np.random.randint(lower_bound, upper_bound+1, size=size).astype(float) mask = np.random.choice([1, 0], a.shape, p=[.1, .9]).astype(bool) a[mask] = np.nan return a size = 6000 df_cbd = pd.DataFrame() df_cbd['bed'] = generate_array_with_random_nan(1, 2, size) df_cbd['bath'] = generate_array_with_random_nan(1, 2, size) df_cbd['area_usable'] = np.random.randint(20, 40, size=size) df_cbd['region'] = 'cbd' df_suburb = pd.DataFrame() df_suburb['bed'] = generate_array_with_random_nan(1, 4, size) df_suburb['bath'] = generate_array_with_random_nan(1, 4, size) df_suburb['area_usable'] = np.random.randint(30, 200, size=size) df_suburb['region'] = 'suburb' df = pd.concat([df_cbd, df_suburb]) df     bed bath area_usable region     0 2 1 33 cbd   1 1 2 23 cbd   2 1 2 33 cbd   3 2 1 26 cbd   4 2 1 28 cbd   5 2 2 36 cbd   6 1 2 38 cbd   7 2 1 23 cbd   8 2 1 36 cbd   9 nan 2 29 cbd    Report missing values I also randomly remove some values to mimic real-world data (read: they are never ready to use), here we will visualize the missing rate of each column.\ndef report_missing(df): cnts = [] cnt_total = len(df) for col in df.columns: cnt_missing = sum(pd.isnull(df[col]) | pd.isna(df[col])) print(\"col: {}, missing: {}%\".format(col, 100.0 * cnt_missing / cnt_total)) cnts.append({ 'column': col, 'missing': 100.0 * cnt_missing / cnt_total }) cnts_df = pd.DataFrame(cnts) sns.barplot(x=cnts_df.missing, y=cnts_df.column, # palette=['r','b'], # data=cnts_df ) return sns report_missing(df) col: bed, missing: 10.266666666666667% col: bath, missing: 9.616666666666667% col: area_usable, missing: 0.0% col: region, missing: 0.0% Data exploration Knowing the missing rate isn’t everything, thus it is also a good idea to explore data in other areas too.\n## missing bed per region df[df.bed.isna()][\"region\"].value_counts(dropna=False) cbd 634 suburb 598 Name: region, dtype: int64  ## missing bath per region df[df.bath.isna()][\"region\"].value_counts(dropna=False) suburb 588 cbd 566 Name: region, dtype: int64  ## explore region df.region.value_counts() suburb 6000 cbd 6000 Name: region, dtype: int64  ## explore bed df.bed.value_counts() 2.0 4050 1.0 4009 4.0 1393 3.0 1316 Name: bed, dtype: int64  ## explore bath df.bath.value_counts() 1.0 4142 2.0 4022 3.0 1393 4.0 1289 Name: bath, dtype: int64  Remove outliers (wouldn’t want your model to have a sub-par performance from skewed data :-P)\n## remove outliers here Create synthetic columns In this step, we create percentile, mean and rank columns to add more data points, so the model can perform better :D\nFirst, we find aggregate percentiles for each groupby set, then add mean and rank columns.\nsynth_columns = { 'bed': { \"region_bath\": ['region', 'bath'] }, 'bath': { \"region_bed\": ['region', 'bed'] } } for column, groupby_levels in synth_columns.items(): for groupby_level_name, groupby_columns in groupby_levels.items(): # percentile aggregates for pctl in [20,50,80,90]: col_name = 'p{}|{}|{}'.format(pctl, groupby_level_name, column) print(\"calculating -- {}\".format(col_name)) df[col_name] = df[groupby_columns+[column]].fillna(0).groupby(groupby_columns)[column].transform(lambda x: x.quantile(pctl/100.0)) # mean impute mean_impute = 'mean|{}|{}'.format(groupby_level_name,column) print(\"calculating -- {}\".format(mean_impute)) df[mean_impute] = df.groupby(groupby_columns)[column].transform('mean') # bed/bath rank rank_impute = column_name = 'rank|{}|{}'.format(groupby_level_name,column) print(\"calculating -- {}\".format(rank_impute)) df[rank_impute] = df.groupby(groupby_columns)[column].rank(method='dense', na_option='bottom') calculating -- p20|region_bath|bed calculating -- p50|region_bath|bed calculating -- p80|region_bath|bed calculating -- p90|region_bath|bed calculating -- mean|region_bath|bed calculating -- rank|region_bath|bed calculating -- p20|region_bed|bath calculating -- p50|region_bed|bath calculating -- p80|region_bed|bath calculating -- p90|region_bed|bath calculating -- mean|region_bed|bath calculating -- rank|region_bed|bath  Coalesce values In this step we fill in values obtained from the previous step – impute time!!\ndef coalesce(df, columns): ''' Implement coalesce of function in colunms. Inputs: df: reference dataframe columns: columns to perform coalesce Returns: df_tmp: pd.Series that is coalesced Example: df_tmp = pd.DataFrame({'a': [1,2,None,None,None,None], 'b': [None,6,None,8,9,None], 'c': [None,10,None,12,None,13]}) df_tmp['new'] = coalesce(df_tmp, ['a','b','c']) print(df_tmp) ''' df_tmp = df[columns[0]] for c in columns[1:]: df_tmp = df_tmp.fillna(df[c]) return df_tmp coalesce_columns = [ 'bed', 'p50|region_bath|bed', # p50|GROUPBY_LESSER_WEIGHT|bed, ... ] df[\"bed_imputed\"] = coalesce(df, coalesce_columns) coalesce_columns = [ 'bath', 'p50|region_bed|bath', # p50|GROUPBY_LESSER_WEIGHT|bath, ... ] df[\"bath_imputed\"] = coalesce(df, coalesce_columns) Report missing values (again) After we impute the values, let’s see how much we are doing better!\nreport_missing(df) col: bed, missing: 10.266666666666667% col: bath, missing: 9.616666666666667% col: area_usable, missing: 0.0% col: region, missing: 0.0% col: p20|region_bath|bed, missing: 0.0% col: p50|region_bath|bed, missing: 0.0% col: p80|region_bath|bed, missing: 0.0% col: p90|region_bath|bed, missing: 0.0% col: mean|region_bath|bed, missing: 9.616666666666667% col: rank|region_bath|bed, missing: 0.0% col: p20|region_bed|bath, missing: 0.0% col: p50|region_bed|bath, missing: 0.0% col: p80|region_bed|bath, missing: 0.0% col: p90|region_bed|bath, missing: 0.0% col: mean|region_bed|bath, missing: 10.266666666666667% col: rank|region_bed|bath, missing: 0.0% col: bed_imputed, missing: 0.0% col: bath_imputed, missing: 0.0%  Notice that the imputed columns there are no missing values. Yay!\nAssign partition In this step, we partition the data into three sets: train, dev and test. Normally we only split into train and test set, but the additional “dev” set is there so we can make sure it’s not too overfit or underfit.\n## assign partition def assign_partition(x): if x in [0,1,2,3,4,5]: return 0 elif x in [6,7]: return 1 else: return 2 ## assign random id df['listing_id'] = [randint(1000000, 9999999) for i in range(len(df))] ## hashing df[\"hash_id\"] = df[\"listing_id\"].apply(lambda x: x % 10) ## assign partition df[\"partition_id\"] = df[\"hash_id\"].apply(lambda x: assign_partition(x)) ## define columns group y_column = 'area_usable' categ_columns = ['region'] numer_columns = [ 'bed_imputed', 'bath_imputed', 'p20|region_bath|bed', 'p50|region_bath|bed', 'p80|region_bath|bed', 'p90|region_bath|bed', 'mean|region_bath|bed', 'rank|region_bath|bed', 'p20|region_bed|bath', 'p50|region_bed|bath', 'p80|region_bed|bath', 'p90|region_bed|bath', 'mean|region_bed|bath', 'rank|region_bed|bath', ] id_columns = [ 'listing_id', 'hash_id', 'partition_id' ] ## remove missing y df = df.dropna(subset=[y_column]) ## split into train-dev-test df_train = df[df[\"partition_id\"] == 0] df_dev = df[df[\"partition_id\"] == 1] df_test = df[df[\"partition_id\"] == 2] ## split each set into x and y y_train = df_train[y_column].values df_train = df_train[numer_columns+categ_columns] y_dev = df_dev[y_column].values df_dev = df_dev[numer_columns+categ_columns] y_test = df_test[y_column].values df_test = df_test[numer_columns+categ_columns] Create sklearn pipelines Sklearn pipelines is a set of transformers applied on data, plus a final estimator (for statistical model). For instance, you have a transformation step to encode categorical values to integers, if you do this from scratch every time, there’s no guarantee that the integers mapping would be the same. Using a pipelines ensures that it retains the same value-integer mapping for subsequent runs. Additionally, applying each transformation and estimators manually each time can be time consuming, and prone to errors. Think of it as a shortcut to do steps required for a task.\nIn this step, we chain a few pipelines together to process the dataset for the final time. In this example, we use median followed by standard scalar for numeric columns, and mode followed by encoding labels for categorical columns.\n## define pipelines impute_median = SimpleImputer(strategy='median') impute_mode = SimpleImputer(strategy='most_frequent') num_pipeline = Pipeline([ ('impute_median', impute_median), ('std_scaler', StandardScaler()), ]) categ_pipeline = Pipeline([ ('impute_mode', impute_mode), ('categ_1hot', OneHotEncoder(handle_unknown='ignore')), ]) full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, numer_columns), (\"cat\", categ_pipeline, categ_columns), ]) ## fit and transform X_train = full_pipeline.fit_transform(df_train) X_dev = full_pipeline.transform(df_dev) X_test = full_pipeline.transform(df_test) X_train array([[ 0.04673184, 0.06391404, 0. , ..., -0.16000115, 1. , 0. ], [-0.97000929, -0.97263688, 0. , ..., -1.01065389, 1. , 0. ], [ 0.04673184, 0.06391404, 0. , ..., -0.16000115, 1. , 0. ], ..., [-0.97000929, 1.10046497, 0. , ..., 0.69065159, 0. , 1. ], [ 0.04673184, 1.10046497, 0. , ..., 0.69065159, 0. , 1. ], [ 1.06347297, 2.13701589, 0. , ..., 1.54130432, 0. , 1. ]])  Hyperparameter tuning In this step, we try to use different models and parameters to see which performs the best. We utilize mlflow for logging and hyperopt to help with tuning. In this example, we run the trials for 40 iterations, each using a different combination of model and parameters.\n## mlflow + hyperopt combo def objective(params): regressor_type = params['type'] del params['type'] if regressor_type == 'gradient_boosting_regression': estimator = GradientBoostingRegressor(**params) elif regressor_type == 'random_forest_regression': estimator = RandomForestRegressor(**params) elif regressor_type == 'extra_trees_regression': estimator = ExtraTreesRegressor(**params) elif regressor_type == 'decision_tree_regression': estimator = DecisionTreeRegressor(**params) else: return 0 estimator.fit(X_train, y_train) # mae y_dev_hat = estimator.predict(X_dev) mae = median_absolute_error(y_dev, y_dev_hat) # logging with mlflow.start_run(): mlflow.log_param(\"regressor\", estimator.__class__.__name__) # mlflow.log_param(\"params\", params) mlflow.log_param('n_estimators', params.get('n_estimators')) mlflow.log_param('max_depth', params.get('max_depth')) mlflow.log_metric(\"median_absolute_error\", mae) return {'loss': mae, 'status': STATUS_OK} space = hp.choice('regressor_type', [ { 'type': 'gradient_boosting_regression', 'n_estimators': hp.choice('n_estimators1', range(100,200,50)), 'max_depth': hp.choice('max_depth1', range(10,13,1)) }, { 'type': 'random_forest_regression', 'n_estimators': hp.choice('n_estimators2', range(100,200,50)), 'max_depth': hp.choice('max_depth2', range(3,25,1)), 'n_jobs': -1 }, { 'type': 'extra_trees_regression', 'n_estimators': hp.choice('n_estimators3', range(100,200,50)), 'max_depth': hp.choice('max_depth3', range(3,10,2)) }, { 'type': 'decision_tree_regression', 'max_depth': hp.choice('max_depth4', range(3,10,2)) } ]) trials = Trials() max_evals = 40 best = fmin( fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials) print(\"Found minimum after {}trials:\".format(max_evals)) from pprint import pprint pprint(best) 100%|██████████| 40/40 [00:19 Evaluate performance Run “mlflow server” to see the loggin dashboard. There, we can see that RandomForestRegressor has the best performance (the less MAE the better) when using max_depth=4 and n_estimators=150, to test the model’s performance against another test set:\n## use best params on TEST set estimator = RandomForestRegressor(max_depth=4, n_estimators=150) estimator.fit(X_train, y_train) y_train_hat = estimator.predict(X_train) train_mae = median_absolute_error(y_train, y_train_hat) y_dev_hat = estimator.predict(X_dev) dev_mae = median_absolute_error(y_dev, y_dev_hat) y_test_hat = estimator.predict(X_test) test_mae = median_absolute_error(y_test, y_test_hat) mae = { 'name': estimator.__class__.__name__, 'train_mae': train_mae, 'dev_mae': dev_mae, 'test_mae': test_mae } mae = pd.DataFrame([mae]).set_index('name') mae    name train_mae dev_mae test_mae     DecisionTreeRegressor 8.930245 8.592484 8.729826    You’ll notice that we use “median absolute error” to measure performance. There are other metrics available, such as mean squared error, but in some cases it’s more meaningful to use a metric that measure the performance in actual data’s unit, in this case the error on dev and test set are around 8 units away from its correct value. Since normally we use square meter for area, it means the prediction will be off by about 8 square meters in most cases.\nTo summarize, in this article we try to fill in missing data using statistical methods, utilizing sklearn pipelines for reusable steps and hyperparameter tuning to find optimum configuration for imputation process.\n Baania is an equal opportunity company and our team is recruiting. We respect and seek to empower each individual and support the diverse backgrounds, perspectives, skills and experiences.\n","wordCount":"1777","inLanguage":"en","datePublished":"2021-12-08T17:00:00Z","dateModified":"2021-12-08T17:00:00Z","author":{"@type":"Person","name":"Karn Wong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.baania.com/posts/impute-pipelines/"},"publisher":{"@type":"Organization","name":"Baania Engineering","logo":{"@type":"ImageObject","url":"https://raw.githubusercontent.com/baania/baania.github.io/master/static/favicon.ico"}}}</script>
</head>
<body id=top>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://blog.baania.com/ accesskey=h title="Baania Engineering (Alt + H)">Baania Engineering</a>
<span class=logo-switches>
</span>
</div>
<ul id=menu>
<li>
<a href=https://blog.baania.com/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://blog.baania.com/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
<li>
<a href=https://blog.baania.com/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://blog.baania.com/>Home</a>&nbsp;»&nbsp;<a href=https://blog.baania.com/posts/>Posts</a></div>
<h1 class=post-title>
Impute pipelines
</h1>
<div class=post-meta><span title="2021-12-08 17:00:00 +0000 UTC">December 8, 2021</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Karn Wong
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#generate-data aria-label="Generate data">Generate data</a></li>
<li>
<a href=#report-missing-values aria-label="Report missing values">Report missing values</a></li>
<li>
<a href=#data-exploration aria-label="Data exploration">Data exploration</a></li>
<li>
<a href=#remove-outliers aria-label="Remove outliers">Remove outliers</a></li>
<li>
<a href=#create-synthetic-columns aria-label="Create synthetic columns">Create synthetic columns</a></li>
<li>
<a href=#coalesce-values aria-label="Coalesce values">Coalesce values</a></li>
<li>
<a href=#report-missing-values-again aria-label="Report missing values (again)">Report missing values (again)</a></li>
<li>
<a href=#assign-partition aria-label="Assign partition">Assign partition</a></li>
<li>
<a href=#create-sklearn-pipelines aria-label="Create sklearn pipelines">Create sklearn pipelines</a></li>
<li>
<a href=#hyperparameter-tuning aria-label="Hyperparameter tuning">Hyperparameter tuning</a></li>
<li>
<a href=#evaluate-performance aria-label="Evaluate performance">Evaluate performance</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>Imagine having a dataset that you need to use for training a prediction model, but some of the features are missing. The good news is you don&rsquo;t need to throw some data away, just have to impute them.</p>
<p>Imputation, essentially means &ldquo;replacing missing data with substituted values.&rdquo; Good imputation can improve your model performance, but bad ones can cause a catastrophe. Thus, it should be done carefully and thoughtfully. The process can be complex at times, luckily we have a few python modules that can make the process easier!</p>
<p>Below are steps you can take in order to create an imputation pipeline. Github link <a href=https://github.com/baania/impute-pipelines-example>here!</a></p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> random <span style=color:#f92672>import</span> randint

<span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
<span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np

<span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> OneHotEncoder
<span style=color:#f92672>from</span> sklearn.impute <span style=color:#f92672>import</span> SimpleImputer
<span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> StandardScaler
<span style=color:#f92672>from</span> sklearn.pipeline <span style=color:#f92672>import</span> Pipeline
<span style=color:#f92672>from</span> sklearn.compose <span style=color:#f92672>import</span> ColumnTransformer
<span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
<span style=color:#f92672>from</span> sklearn.tree <span style=color:#f92672>import</span> DecisionTreeRegressor
<span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> mean_squared_error, median_absolute_error

<span style=color:#f92672>from</span> hyperopt <span style=color:#f92672>import</span> fmin, tpe, hp, Trials, STATUS_OK
<span style=color:#f92672>import</span> mlflow

<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
<span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
sns<span style=color:#f92672>.</span>set()
</code></pre></div><h2 id=generate-data>Generate data<a hidden class=anchor aria-hidden=true href=#generate-data>#</a></h2>
<p>Since this is an example and I don&rsquo;t want to get sued by using my company&rsquo;s data, synthetic data it is :)
This simulates a dataset from different pseudo-regions, with different characteristics. Real data will be much more varied, but I make it more obvious so it&rsquo;s easy to see the differences.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_array_with_random_nan</span>(lower_bound, upper_bound, size):
    a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(lower_bound, upper_bound<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>, size<span style=color:#f92672>=</span>size)<span style=color:#f92672>.</span>astype(float)
    mask <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>], a<span style=color:#f92672>.</span>shape, p<span style=color:#f92672>=</span>[<span style=color:#ae81ff>.1</span>, <span style=color:#ae81ff>.9</span>])<span style=color:#f92672>.</span>astype(bool)
    a[mask] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>nan

    <span style=color:#66d9ef>return</span> a

size <span style=color:#f92672>=</span> <span style=color:#ae81ff>6000</span>

df_cbd <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame()
df_cbd[<span style=color:#e6db74>&#39;bed&#39;</span>] <span style=color:#f92672>=</span> generate_array_with_random_nan(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, size)
df_cbd[<span style=color:#e6db74>&#39;bath&#39;</span>] <span style=color:#f92672>=</span> generate_array_with_random_nan(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, size)
df_cbd[<span style=color:#e6db74>&#39;area_usable&#39;</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>40</span>, size<span style=color:#f92672>=</span>size)
df_cbd[<span style=color:#e6db74>&#39;region&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;cbd&#39;</span>

df_suburb <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame()
df_suburb[<span style=color:#e6db74>&#39;bed&#39;</span>] <span style=color:#f92672>=</span> generate_array_with_random_nan(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, size)
df_suburb[<span style=color:#e6db74>&#39;bath&#39;</span>] <span style=color:#f92672>=</span> generate_array_with_random_nan(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, size)
df_suburb[<span style=color:#e6db74>&#39;area_usable&#39;</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>200</span>, size<span style=color:#f92672>=</span>size)
df_suburb[<span style=color:#e6db74>&#39;region&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;suburb&#39;</span>

df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>concat([df_cbd, df_suburb])
df
</code></pre></div><table>
<thead>
<tr>
<th style=text-align:right></th>
<th style=text-align:right>bed</th>
<th style=text-align:right>bath</th>
<th style=text-align:right>area_usable</th>
<th style=text-align:left>region</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:right>0</td>
<td style=text-align:right>2</td>
<td style=text-align:right>1</td>
<td style=text-align:right>33</td>
<td style=text-align:left>cbd</td>
</tr>
<tr>
<td style=text-align:right>1</td>
<td style=text-align:right>1</td>
<td style=text-align:right>2</td>
<td style=text-align:right>23</td>
<td style=text-align:left>cbd</td>
</tr>
<tr>
<td style=text-align:right>2</td>
<td style=text-align:right>1</td>
<td style=text-align:right>2</td>
<td style=text-align:right>33</td>
<td style=text-align:left>cbd</td>
</tr>
<tr>
<td style=text-align:right>3</td>
<td style=text-align:right>2</td>
<td style=text-align:right>1</td>
<td style=text-align:right>26</td>
<td style=text-align:left>cbd</td>
</tr>
<tr>
<td style=text-align:right>4</td>
<td style=text-align:right>2</td>
<td style=text-align:right>1</td>
<td style=text-align:right>28</td>
<td style=text-align:left>cbd</td>
</tr>
<tr>
<td style=text-align:right>5</td>
<td style=text-align:right>2</td>
<td style=text-align:right>2</td>
<td style=text-align:right>36</td>
<td style=text-align:left>cbd</td>
</tr>
<tr>
<td style=text-align:right>6</td>
<td style=text-align:right>1</td>
<td style=text-align:right>2</td>
<td style=text-align:right>38</td>
<td style=text-align:left>cbd</td>
</tr>
<tr>
<td style=text-align:right>7</td>
<td style=text-align:right>2</td>
<td style=text-align:right>1</td>
<td style=text-align:right>23</td>
<td style=text-align:left>cbd</td>
</tr>
<tr>
<td style=text-align:right>8</td>
<td style=text-align:right>2</td>
<td style=text-align:right>1</td>
<td style=text-align:right>36</td>
<td style=text-align:left>cbd</td>
</tr>
<tr>
<td style=text-align:right>9</td>
<td style=text-align:right>nan</td>
<td style=text-align:right>2</td>
<td style=text-align:right>29</td>
<td style=text-align:left>cbd</td>
</tr>
</tbody>
</table>
<h2 id=report-missing-values>Report missing values<a hidden class=anchor aria-hidden=true href=#report-missing-values>#</a></h2>
<p>I also randomly remove some values to mimic real-world data (read: they are never ready to use), here we will visualize the missing rate of each column.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>report_missing</span>(df):
    cnts <span style=color:#f92672>=</span> []
    cnt_total <span style=color:#f92672>=</span> len(df)
    <span style=color:#66d9ef>for</span> col <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns:
        cnt_missing <span style=color:#f92672>=</span> sum(pd<span style=color:#f92672>.</span>isnull(df[col]) <span style=color:#f92672>|</span> pd<span style=color:#f92672>.</span>isna(df[col]))
        print(<span style=color:#e6db74>&#34;col: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>, missing: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>%&#34;</span><span style=color:#f92672>.</span>format(col, <span style=color:#ae81ff>100.0</span> <span style=color:#f92672>*</span> cnt_missing <span style=color:#f92672>/</span> cnt_total))

        cnts<span style=color:#f92672>.</span>append({
            <span style=color:#e6db74>&#39;column&#39;</span>: col,
            <span style=color:#e6db74>&#39;missing&#39;</span>: <span style=color:#ae81ff>100.0</span> <span style=color:#f92672>*</span> cnt_missing <span style=color:#f92672>/</span> cnt_total
        })

    cnts_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(cnts)
    sns<span style=color:#f92672>.</span>barplot(x<span style=color:#f92672>=</span>cnts_df<span style=color:#f92672>.</span>missing,
                y<span style=color:#f92672>=</span>cnts_df<span style=color:#f92672>.</span>column,
    <span style=color:#75715e>#             palette=[&#39;r&#39;,&#39;b&#39;],</span>
    <span style=color:#75715e>#             data=cnts_df</span>
                )

    <span style=color:#66d9ef>return</span> sns

report_missing(df)
</code></pre></div><pre tabindex=0><code>col: bed, missing: 10.266666666666667%
col: bath, missing: 9.616666666666667%
col: area_usable, missing: 0.0%
col: region, missing: 0.0%
</code></pre><p><img loading=lazy src=/images/2021-08-18-19-05-52.png alt>
</p>
<h2 id=data-exploration>Data exploration<a hidden class=anchor aria-hidden=true href=#data-exploration>#</a></h2>
<p>Knowing the missing rate isn&rsquo;t everything, thus it is also a good idea to explore data in other areas too.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## missing bed per region</span>
df[df<span style=color:#f92672>.</span>bed<span style=color:#f92672>.</span>isna()][<span style=color:#e6db74>&#34;region&#34;</span>]<span style=color:#f92672>.</span>value_counts(dropna<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</code></pre></div><pre><code>cbd       634
suburb    598
Name: region, dtype: int64
</code></pre>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## missing bath per region</span>
df[df<span style=color:#f92672>.</span>bath<span style=color:#f92672>.</span>isna()][<span style=color:#e6db74>&#34;region&#34;</span>]<span style=color:#f92672>.</span>value_counts(dropna<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</code></pre></div><pre><code>suburb    588
cbd       566
Name: region, dtype: int64
</code></pre>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## explore region</span>
df<span style=color:#f92672>.</span>region<span style=color:#f92672>.</span>value_counts()
</code></pre></div><pre><code>suburb    6000
cbd       6000
Name: region, dtype: int64
</code></pre>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## explore bed</span>
df<span style=color:#f92672>.</span>bed<span style=color:#f92672>.</span>value_counts()
</code></pre></div><pre><code>2.0    4050
1.0    4009
4.0    1393
3.0    1316
Name: bed, dtype: int64
</code></pre>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## explore bath</span>
df<span style=color:#f92672>.</span>bath<span style=color:#f92672>.</span>value_counts()
</code></pre></div><pre><code>1.0    4142
2.0    4022
3.0    1393
4.0    1289
Name: bath, dtype: int64
</code></pre>
<h2 id=remove-outliers>Remove outliers<a hidden class=anchor aria-hidden=true href=#remove-outliers>#</a></h2>
<p>(wouldn&rsquo;t want your model to have a sub-par performance from skewed data :-P)</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## remove outliers here</span>
</code></pre></div><h2 id=create-synthetic-columns>Create synthetic columns<a hidden class=anchor aria-hidden=true href=#create-synthetic-columns>#</a></h2>
<p>In this step, we create percentile, mean and rank columns to add more data points, so the model can perform better :D</p>
<p>First, we find aggregate percentiles for each groupby set, then add mean and rank columns.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>synth_columns <span style=color:#f92672>=</span> {
    <span style=color:#e6db74>&#39;bed&#39;</span>: {
        <span style=color:#e6db74>&#34;region_bath&#34;</span>: [<span style=color:#e6db74>&#39;region&#39;</span>, <span style=color:#e6db74>&#39;bath&#39;</span>]
    },
    <span style=color:#e6db74>&#39;bath&#39;</span>: {
        <span style=color:#e6db74>&#34;region_bed&#34;</span>: [<span style=color:#e6db74>&#39;region&#39;</span>, <span style=color:#e6db74>&#39;bed&#39;</span>]
    }
}

<span style=color:#66d9ef>for</span> column, groupby_levels <span style=color:#f92672>in</span> synth_columns<span style=color:#f92672>.</span>items():
    <span style=color:#66d9ef>for</span> groupby_level_name, groupby_columns <span style=color:#f92672>in</span> groupby_levels<span style=color:#f92672>.</span>items():
        <span style=color:#75715e># percentile aggregates</span>
        <span style=color:#66d9ef>for</span> pctl <span style=color:#f92672>in</span> [<span style=color:#ae81ff>20</span>,<span style=color:#ae81ff>50</span>,<span style=color:#ae81ff>80</span>,<span style=color:#ae81ff>90</span>]:
            col_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;p</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(pctl, groupby_level_name, column)
            print(<span style=color:#e6db74>&#34;calculating -- </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(col_name))
            df[col_name] <span style=color:#f92672>=</span> df[groupby_columns<span style=color:#f92672>+</span>[column]]<span style=color:#f92672>.</span>fillna(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>groupby(groupby_columns)[column]<span style=color:#f92672>.</span>transform(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>.</span>quantile(pctl<span style=color:#f92672>/</span><span style=color:#ae81ff>100.0</span>))

        <span style=color:#75715e># mean impute</span>
        mean_impute <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;mean|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(groupby_level_name,column)
        print(<span style=color:#e6db74>&#34;calculating -- </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(mean_impute))
        df[mean_impute] <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>groupby(groupby_columns)[column]<span style=color:#f92672>.</span>transform(<span style=color:#e6db74>&#39;mean&#39;</span>)

        <span style=color:#75715e># bed/bath rank</span>
        rank_impute <span style=color:#f92672>=</span> column_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;rank|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>|</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(groupby_level_name,column)
        print(<span style=color:#e6db74>&#34;calculating -- </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(rank_impute))
        df[rank_impute] <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>groupby(groupby_columns)[column]<span style=color:#f92672>.</span>rank(method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dense&#39;</span>, na_option<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bottom&#39;</span>)

</code></pre></div><pre><code>calculating -- p20|region_bath|bed
calculating -- p50|region_bath|bed
calculating -- p80|region_bath|bed
calculating -- p90|region_bath|bed
calculating -- mean|region_bath|bed
calculating -- rank|region_bath|bed
calculating -- p20|region_bed|bath
calculating -- p50|region_bed|bath
calculating -- p80|region_bed|bath
calculating -- p90|region_bed|bath
calculating -- mean|region_bed|bath
calculating -- rank|region_bed|bath
</code></pre>
<h2 id=coalesce-values>Coalesce values<a hidden class=anchor aria-hidden=true href=#coalesce-values>#</a></h2>
<p>In this step we fill in values obtained from the previous step &ndash; impute time!!</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>coalesce</span>(df, columns):
    <span style=color:#e6db74>&#39;&#39;&#39;
</span><span style=color:#e6db74>    Implement coalesce of function in colunms.
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>    Inputs:
</span><span style=color:#e6db74>    df: reference dataframe
</span><span style=color:#e6db74>    columns: columns to perform coalesce
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>    Returns:
</span><span style=color:#e6db74>    df_tmp: pd.Series that is coalesced
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>    Example:
</span><span style=color:#e6db74>    df_tmp = pd.DataFrame({&#39;a&#39;: [1,2,None,None,None,None],
</span><span style=color:#e6db74>                            &#39;b&#39;: [None,6,None,8,9,None],
</span><span style=color:#e6db74>                            &#39;c&#39;: [None,10,None,12,None,13]})
</span><span style=color:#e6db74>    df_tmp[&#39;new&#39;] = coalesce(df_tmp, [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;])
</span><span style=color:#e6db74>    print(df_tmp)
</span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
    df_tmp <span style=color:#f92672>=</span> df[columns[<span style=color:#ae81ff>0</span>]]
    <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> columns[<span style=color:#ae81ff>1</span>:]:
        df_tmp <span style=color:#f92672>=</span> df_tmp<span style=color:#f92672>.</span>fillna(df[c])

    <span style=color:#66d9ef>return</span> df_tmp


coalesce_columns <span style=color:#f92672>=</span> [
    <span style=color:#e6db74>&#39;bed&#39;</span>,
    <span style=color:#e6db74>&#39;p50|region_bath|bed&#39;</span>,
    <span style=color:#75715e># p50|GROUPBY_LESSER_WEIGHT|bed, ...</span>
]

df[<span style=color:#e6db74>&#34;bed_imputed&#34;</span>] <span style=color:#f92672>=</span> coalesce(df, coalesce_columns)

coalesce_columns <span style=color:#f92672>=</span> [
    <span style=color:#e6db74>&#39;bath&#39;</span>,
    <span style=color:#e6db74>&#39;p50|region_bed|bath&#39;</span>,
     <span style=color:#75715e># p50|GROUPBY_LESSER_WEIGHT|bath, ...</span>
]

df[<span style=color:#e6db74>&#34;bath_imputed&#34;</span>] <span style=color:#f92672>=</span> coalesce(df, coalesce_columns)
</code></pre></div><h2 id=report-missing-values-again>Report missing values (again)<a hidden class=anchor aria-hidden=true href=#report-missing-values-again>#</a></h2>
<p>After we impute the values, let&rsquo;s see how much we are doing better!</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>report_missing(df)
</code></pre></div><pre><code>col: bed, missing: 10.266666666666667%
col: bath, missing: 9.616666666666667%
col: area_usable, missing: 0.0%
col: region, missing: 0.0%
col: p20|region_bath|bed, missing: 0.0%
col: p50|region_bath|bed, missing: 0.0%
col: p80|region_bath|bed, missing: 0.0%
col: p90|region_bath|bed, missing: 0.0%
col: mean|region_bath|bed, missing: 9.616666666666667%
col: rank|region_bath|bed, missing: 0.0%
col: p20|region_bed|bath, missing: 0.0%
col: p50|region_bed|bath, missing: 0.0%
col: p80|region_bed|bath, missing: 0.0%
col: p90|region_bed|bath, missing: 0.0%
col: mean|region_bed|bath, missing: 10.266666666666667%
col: rank|region_bed|bath, missing: 0.0%
col: bed_imputed, missing: 0.0%
col: bath_imputed, missing: 0.0%
</code></pre>
<p><img loading=lazy src=/images/2021-08-18-19-08-00.png alt>
</p>
<p>Notice that the imputed columns there are no missing values. Yay!</p>
<h2 id=assign-partition>Assign partition<a hidden class=anchor aria-hidden=true href=#assign-partition>#</a></h2>
<p>In this step, we partition the data into three sets: train, dev and test. Normally we only split into train and test set, but the additional &ldquo;dev&rdquo; set is there so we can make sure it&rsquo;s not too overfit or underfit.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## assign partition</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>assign_partition</span>(x):
    <span style=color:#66d9ef>if</span> x <span style=color:#f92672>in</span> [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>]:
        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>
    <span style=color:#66d9ef>elif</span> x <span style=color:#f92672>in</span> [<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>]:
        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span>
    <span style=color:#66d9ef>else</span>:
        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2</span>

<span style=color:#75715e>## assign random id</span>
df[<span style=color:#e6db74>&#39;listing_id&#39;</span>] <span style=color:#f92672>=</span> [randint(<span style=color:#ae81ff>1000000</span>, <span style=color:#ae81ff>9999999</span>) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(df))]

<span style=color:#75715e>## hashing</span>
df[<span style=color:#e6db74>&#34;hash_id&#34;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#34;listing_id&#34;</span>]<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> x: x <span style=color:#f92672>%</span> <span style=color:#ae81ff>10</span>)

<span style=color:#75715e>## assign partition</span>
df[<span style=color:#e6db74>&#34;partition_id&#34;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#34;hash_id&#34;</span>]<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> x: assign_partition(x))
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## define columns group</span>
y_column <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;area_usable&#39;</span>

categ_columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;region&#39;</span>]

numer_columns <span style=color:#f92672>=</span> [
    <span style=color:#e6db74>&#39;bed_imputed&#39;</span>,
    <span style=color:#e6db74>&#39;bath_imputed&#39;</span>,

    <span style=color:#e6db74>&#39;p20|region_bath|bed&#39;</span>,
    <span style=color:#e6db74>&#39;p50|region_bath|bed&#39;</span>,
    <span style=color:#e6db74>&#39;p80|region_bath|bed&#39;</span>,
    <span style=color:#e6db74>&#39;p90|region_bath|bed&#39;</span>,
    <span style=color:#e6db74>&#39;mean|region_bath|bed&#39;</span>,
    <span style=color:#e6db74>&#39;rank|region_bath|bed&#39;</span>,

    <span style=color:#e6db74>&#39;p20|region_bed|bath&#39;</span>,
    <span style=color:#e6db74>&#39;p50|region_bed|bath&#39;</span>,
    <span style=color:#e6db74>&#39;p80|region_bed|bath&#39;</span>,
    <span style=color:#e6db74>&#39;p90|region_bed|bath&#39;</span>,
    <span style=color:#e6db74>&#39;mean|region_bed|bath&#39;</span>,
    <span style=color:#e6db74>&#39;rank|region_bed|bath&#39;</span>,
]

id_columns <span style=color:#f92672>=</span> [
    <span style=color:#e6db74>&#39;listing_id&#39;</span>,
    <span style=color:#e6db74>&#39;hash_id&#39;</span>,
    <span style=color:#e6db74>&#39;partition_id&#39;</span>
]
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## remove missing y</span>
df <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>dropna(subset<span style=color:#f92672>=</span>[y_column])

<span style=color:#75715e>## split into train-dev-test</span>
df_train <span style=color:#f92672>=</span> df[df[<span style=color:#e6db74>&#34;partition_id&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>]
df_dev <span style=color:#f92672>=</span> df[df[<span style=color:#e6db74>&#34;partition_id&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>]
df_test <span style=color:#f92672>=</span> df[df[<span style=color:#e6db74>&#34;partition_id&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span>]
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## split each set into x and y</span>
y_train <span style=color:#f92672>=</span> df_train[y_column]<span style=color:#f92672>.</span>values
df_train <span style=color:#f92672>=</span> df_train[numer_columns<span style=color:#f92672>+</span>categ_columns]

y_dev <span style=color:#f92672>=</span> df_dev[y_column]<span style=color:#f92672>.</span>values
df_dev <span style=color:#f92672>=</span> df_dev[numer_columns<span style=color:#f92672>+</span>categ_columns]

y_test <span style=color:#f92672>=</span> df_test[y_column]<span style=color:#f92672>.</span>values
df_test <span style=color:#f92672>=</span> df_test[numer_columns<span style=color:#f92672>+</span>categ_columns]
</code></pre></div><h2 id=create-sklearn-pipelines>Create sklearn pipelines<a hidden class=anchor aria-hidden=true href=#create-sklearn-pipelines>#</a></h2>
<p>Sklearn pipelines is a set of transformers applied on data, plus a final estimator (for statistical model). For instance, you have a transformation step to encode categorical values to integers, if you do this from scratch every time, there&rsquo;s no guarantee that the integers mapping would be the same. Using a pipelines ensures that it retains the same value-integer mapping for subsequent runs. Additionally, applying each transformation and estimators manually each time can be time consuming, and prone to errors. Think of it as a shortcut to do steps required for a task.</p>
<p>In this step, we chain a few pipelines together to process the dataset for the final time. In this example, we use median followed by standard scalar for numeric columns, and mode followed by encoding labels for categorical columns.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## define pipelines</span>
impute_median <span style=color:#f92672>=</span> SimpleImputer(strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;median&#39;</span>)
impute_mode <span style=color:#f92672>=</span> SimpleImputer(strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;most_frequent&#39;</span>)

num_pipeline <span style=color:#f92672>=</span> Pipeline([
        (<span style=color:#e6db74>&#39;impute_median&#39;</span>, impute_median),
        (<span style=color:#e6db74>&#39;std_scaler&#39;</span>, StandardScaler()),
    ])

categ_pipeline <span style=color:#f92672>=</span> Pipeline([
        (<span style=color:#e6db74>&#39;impute_mode&#39;</span>, impute_mode),
        (<span style=color:#e6db74>&#39;categ_1hot&#39;</span>, OneHotEncoder(handle_unknown<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ignore&#39;</span>)),
    ])

full_pipeline <span style=color:#f92672>=</span> ColumnTransformer([
        (<span style=color:#e6db74>&#34;num&#34;</span>, num_pipeline, numer_columns),
        (<span style=color:#e6db74>&#34;cat&#34;</span>, categ_pipeline, categ_columns),
    ])
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## fit and transform</span>
X_train <span style=color:#f92672>=</span> full_pipeline<span style=color:#f92672>.</span>fit_transform(df_train)
X_dev <span style=color:#f92672>=</span> full_pipeline<span style=color:#f92672>.</span>transform(df_dev)
X_test <span style=color:#f92672>=</span> full_pipeline<span style=color:#f92672>.</span>transform(df_test)
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>X_train
</code></pre></div><pre><code>array([[ 0.04673184,  0.06391404,  0.        , ..., -0.16000115,
         1.        ,  0.        ],
       [-0.97000929, -0.97263688,  0.        , ..., -1.01065389,
         1.        ,  0.        ],
       [ 0.04673184,  0.06391404,  0.        , ..., -0.16000115,
         1.        ,  0.        ],
       ...,
       [-0.97000929,  1.10046497,  0.        , ...,  0.69065159,
         0.        ,  1.        ],
       [ 0.04673184,  1.10046497,  0.        , ...,  0.69065159,
         0.        ,  1.        ],
       [ 1.06347297,  2.13701589,  0.        , ...,  1.54130432,
         0.        ,  1.        ]])
</code></pre>
<h2 id=hyperparameter-tuning>Hyperparameter tuning<a hidden class=anchor aria-hidden=true href=#hyperparameter-tuning>#</a></h2>
<p>In this step, we try to use different models and parameters to see which performs the best. We utilize mlflow for logging and hyperopt to help with tuning. In this example, we run the trials for 40 iterations, each using a different combination of model and parameters.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## mlflow + hyperopt combo</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>objective</span>(params):
    regressor_type <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#39;type&#39;</span>]
    <span style=color:#66d9ef>del</span> params[<span style=color:#e6db74>&#39;type&#39;</span>]
    <span style=color:#66d9ef>if</span> regressor_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;gradient_boosting_regression&#39;</span>:
        estimator <span style=color:#f92672>=</span> GradientBoostingRegressor(<span style=color:#f92672>**</span>params)
    <span style=color:#66d9ef>elif</span> regressor_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;random_forest_regression&#39;</span>:
        estimator <span style=color:#f92672>=</span> RandomForestRegressor(<span style=color:#f92672>**</span>params)
    <span style=color:#66d9ef>elif</span> regressor_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;extra_trees_regression&#39;</span>:
        estimator <span style=color:#f92672>=</span> ExtraTreesRegressor(<span style=color:#f92672>**</span>params)
    <span style=color:#66d9ef>elif</span> regressor_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;decision_tree_regression&#39;</span>:
        estimator <span style=color:#f92672>=</span> DecisionTreeRegressor(<span style=color:#f92672>**</span>params)
    <span style=color:#66d9ef>else</span>:
        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>

    estimator<span style=color:#f92672>.</span>fit(X_train, y_train)

    <span style=color:#75715e># mae</span>
    y_dev_hat <span style=color:#f92672>=</span> estimator<span style=color:#f92672>.</span>predict(X_dev)
    mae <span style=color:#f92672>=</span> median_absolute_error(y_dev, y_dev_hat)

    <span style=color:#75715e># logging</span>
    <span style=color:#66d9ef>with</span> mlflow<span style=color:#f92672>.</span>start_run():
        mlflow<span style=color:#f92672>.</span>log_param(<span style=color:#e6db74>&#34;regressor&#34;</span>, estimator<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__)
        <span style=color:#75715e># mlflow.log_param(&#34;params&#34;, params)</span>
        mlflow<span style=color:#f92672>.</span>log_param(<span style=color:#e6db74>&#39;n_estimators&#39;</span>, params<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;n_estimators&#39;</span>))
        mlflow<span style=color:#f92672>.</span>log_param(<span style=color:#e6db74>&#39;max_depth&#39;</span>, params<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;max_depth&#39;</span>))

        mlflow<span style=color:#f92672>.</span>log_metric(<span style=color:#e6db74>&#34;median_absolute_error&#34;</span>, mae)

    <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#39;loss&#39;</span>: mae, <span style=color:#e6db74>&#39;status&#39;</span>: STATUS_OK}

space <span style=color:#f92672>=</span> hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;regressor_type&#39;</span>, [
    {
        <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;gradient_boosting_regression&#39;</span>,
        <span style=color:#e6db74>&#39;n_estimators&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;n_estimators1&#39;</span>, range(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>200</span>,<span style=color:#ae81ff>50</span>)),
        <span style=color:#e6db74>&#39;max_depth&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;max_depth1&#39;</span>, range(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>13</span>,<span style=color:#ae81ff>1</span>))
    },
    {
        <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;random_forest_regression&#39;</span>,
        <span style=color:#e6db74>&#39;n_estimators&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;n_estimators2&#39;</span>, range(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>200</span>,<span style=color:#ae81ff>50</span>)),
        <span style=color:#e6db74>&#39;max_depth&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;max_depth2&#39;</span>, range(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>25</span>,<span style=color:#ae81ff>1</span>)),
        <span style=color:#e6db74>&#39;n_jobs&#39;</span>: <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>

    },
    {
        <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;extra_trees_regression&#39;</span>,
        <span style=color:#e6db74>&#39;n_estimators&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;n_estimators3&#39;</span>, range(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>200</span>,<span style=color:#ae81ff>50</span>)),
        <span style=color:#e6db74>&#39;max_depth&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;max_depth3&#39;</span>, range(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>2</span>))
    },
    {
        <span style=color:#e6db74>&#39;type&#39;</span>: <span style=color:#e6db74>&#39;decision_tree_regression&#39;</span>,
        <span style=color:#e6db74>&#39;max_depth&#39;</span>: hp<span style=color:#f92672>.</span>choice(<span style=color:#e6db74>&#39;max_depth4&#39;</span>, range(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>2</span>))
    }
])

trials <span style=color:#f92672>=</span> Trials()
max_evals <span style=color:#f92672>=</span> <span style=color:#ae81ff>40</span>

best <span style=color:#f92672>=</span> fmin(
fn<span style=color:#f92672>=</span>objective,
space<span style=color:#f92672>=</span>space,
algo<span style=color:#f92672>=</span>tpe<span style=color:#f92672>.</span>suggest,
max_evals<span style=color:#f92672>=</span>max_evals,
trials<span style=color:#f92672>=</span>trials)

print(<span style=color:#e6db74>&#34;Found minimum after </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> trials:&#34;</span><span style=color:#f92672>.</span>format(max_evals))
<span style=color:#f92672>from</span> pprint <span style=color:#f92672>import</span> pprint
pprint(best)
</code></pre></div><pre><code>100%|██████████| 40/40 [00:19&lt;00:00,  2.11trial/s, best loss: 8.569474762575908]
Found minimum after 40 trials:
{'max_depth2': 1, 'n_estimators2': 1, 'regressor_type': 1}
</code></pre>
<h2 id=evaluate-performance>Evaluate performance<a hidden class=anchor aria-hidden=true href=#evaluate-performance>#</a></h2>
<p>Run &ldquo;mlflow server&rdquo; to see the loggin dashboard. There, we can see that RandomForestRegressor has the best performance (the less MAE the better) when using max_depth=4 and n_estimators=150, to test the model&rsquo;s performance against another test set:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>## use best params on TEST set</span>
estimator <span style=color:#f92672>=</span> RandomForestRegressor(max_depth<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>150</span>)
estimator<span style=color:#f92672>.</span>fit(X_train, y_train)

y_train_hat <span style=color:#f92672>=</span> estimator<span style=color:#f92672>.</span>predict(X_train)
train_mae <span style=color:#f92672>=</span> median_absolute_error(y_train, y_train_hat)

y_dev_hat <span style=color:#f92672>=</span> estimator<span style=color:#f92672>.</span>predict(X_dev)
dev_mae <span style=color:#f92672>=</span> median_absolute_error(y_dev, y_dev_hat)

y_test_hat <span style=color:#f92672>=</span> estimator<span style=color:#f92672>.</span>predict(X_test)
test_mae <span style=color:#f92672>=</span> median_absolute_error(y_test, y_test_hat)

mae <span style=color:#f92672>=</span>  {
    <span style=color:#e6db74>&#39;name&#39;</span>: estimator<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__,
    <span style=color:#e6db74>&#39;train_mae&#39;</span>: train_mae,
    <span style=color:#e6db74>&#39;dev_mae&#39;</span>: dev_mae,
    <span style=color:#e6db74>&#39;test_mae&#39;</span>: test_mae
}

mae <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame([mae])<span style=color:#f92672>.</span>set_index(<span style=color:#e6db74>&#39;name&#39;</span>)

mae
</code></pre></div><table>
<thead>
<tr>
<th style=text-align:left>name</th>
<th style=text-align:right>train_mae</th>
<th style=text-align:right>dev_mae</th>
<th style=text-align:right>test_mae</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left>DecisionTreeRegressor</td>
<td style=text-align:right>8.930245</td>
<td style=text-align:right>8.592484</td>
<td style=text-align:right>8.729826</td>
</tr>
</tbody>
</table>
<p>You&rsquo;ll notice that we use &ldquo;median absolute error&rdquo; to measure performance. There are other metrics available, such as mean squared error, but in some cases it&rsquo;s more meaningful to use a metric that measure the performance in actual data&rsquo;s unit, in this case the error on dev and test set are around 8 units away from its correct value. Since normally we use square meter for area, it means the prediction will be off by about 8 square meters in most cases.</p>
<p>To summarize, in this article we try to fill in missing data using statistical methods, utilizing sklearn pipelines for reusable steps and hyperparameter tuning to find optimum configuration for imputation process.</p>
<hr>
<p>Baania is an equal opportunity company and our team is recruiting. We respect and seek to empower each individual and support the diverse backgrounds, perspectives, skills and experiences.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://blog.baania.com/tags/data-science/>data science</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://blog.baania.com/posts/secrets-management-with-sops-aws-ssm-terraform/>
<span class=title>« Prev Page</span>
<br>
<span>Secrets management with SOPS, AWS SSM and Terraform</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Impute pipelines on twitter" href="https://twitter.com/intent/tweet/?text=Impute%20pipelines&url=https%3a%2f%2fblog.baania.com%2fposts%2fimpute-pipelines%2f&hashtags=datascience"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Impute pipelines on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fblog.baania.com%2fposts%2fimpute-pipelines%2f&title=Impute%20pipelines&summary=Impute%20pipelines&source=https%3a%2f%2fblog.baania.com%2fposts%2fimpute-pipelines%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Impute pipelines on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblog.baania.com%2fposts%2fimpute-pipelines%2f&title=Impute%20pipelines"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Impute pipelines on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.baania.com%2fposts%2fimpute-pipelines%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Impute pipelines on whatsapp" href="https://api.whatsapp.com/send?text=Impute%20pipelines%20-%20https%3a%2f%2fblog.baania.com%2fposts%2fimpute-pipelines%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Impute pipelines on telegram" href="https://telegram.me/share/url?text=Impute%20pipelines&url=https%3a%2f%2fblog.baania.com%2fposts%2fimpute-pipelines%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://blog.baania.com/>Baania Engineering</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>